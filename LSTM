import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import glob
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from datetime import datetime

# Function to preprocess data and train model
def process_and_train_model(file_path, split_date):
    # Load data
    df = pd.read_csv(file_path)

    # Convert date column to datetime
    df['Date'] = pd.to_datetime(df['Date'])

    # Preprocess data
    closing_prices = df['Close'].values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(closing_prices)

    X, y = [], []
    time_steps = 60
    for i in range(time_steps, len(scaled_data)):
        X.append(scaled_data[i-time_steps:i, 0])
        y.append(scaled_data[i, 0])

    X, y = np.array(X), np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # Find the index for the split date
    split_index = df[df['Date'] >= split_date].index[0]

    # Split data based on the index
    X_train, X_test = X[:split_index], X[split_index:]
    y_train, y_test = y[:split_index], y[split_index:]

    # Build model
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
    model.add(Dropout(0.2))
    model.add(LSTM(units=50, return_sequences=False))
    model.add(Dropout(0.2))
    model.add(Dense(units=1))

    model.compile(optimizer='adam', loss='mean_squared_error')

    # Train model
    model.fit(X_train, y_train, epochs=100, batch_size=32)

    # Predict
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = scaler.inverse_transform(predicted_stock_price)

    # Evaluate
    real_stock_price = scaler.inverse_transform(y_test.reshape(-1, 1))
    rmse = np.sqrt(mean_squared_error(real_stock_price, predicted_stock_price))
    print(f"Root Mean Squared Error for {os.path.basename(file_path)}: {rmse}")

    # Extract dates for the test data
    test_dates = df[df['Date'] >= split_date]['Date']
    test_dates = test_dates.iloc[:len(y_test)].reset_index(drop=True)

    # Convert predictions and actual prices back to DataFrame
    test_results = pd.DataFrame({
        'Actual_Price': real_stock_price.ravel(),
        'Predicted_Price': predicted_stock_price.ravel()
    })

    # Save to CSV
    test_results.to_csv(f"test_results_{os.path.basename(file_path)}", index=False)

    # Plotting
    plt.figure(figsize=(12,6))
    plt.plot(real_stock_price, color='red', label='Actual Stock Price')
    plt.plot(predicted_stock_price, color='blue', label='Predicted Stock Price')
    plt.title(f'Stock Price Prediction for {os.path.basename(file_path)}')
    plt.xlabel('Time')
    plt.ylabel('Stock Price')
    plt.legend()
    plt.show()

# Process each CSV file
path = 'newTest'  # Update to your CSV files directory
all_files = glob.glob(os.path.join(path, "*.csv"))

# Example split date
split_date = datetime(2018, 10, 4)  # Change to your desired split date

for filename in all_files:
    process_and_train_model(filename, split_date)
